# DataBaseSpider 项目详细说明文档
## 一、项目概述
### 项目定位
该项目是分布式数据库小组作业，基于 Scrapy 爬虫框架开发，核心功能为爬取拉勾网相关数据（职位分类、职位详情、公司信息），并实现数据的存储、导出与处理。

### 核心技术栈
- 爬虫框架：Scrapy
- 数据库：MongoDB（主存储）、Redis（临时存储）
- 数据处理：Python、pandas
- 反爬支持：User-Agent 随机切换、Cookie 动态配置、延时访问

## 二、项目结构
```
DataBaseSpider-master/
├── .idea/                  # 项目配置文件目录
├── DataProcess/            # 数据处理模块
│   ├── DataProcessing.py   # CSV缺失值填充/删除
│   └── MongodbToCsv.py     # MongoDB数据导出为CSV
├── MongoDb2Csv/            # MongoDB转CSV功能模块
├── NonScrapy/              # 非Scrapy爬虫相关代码
├── lagou/                  # 拉勾网爬虫核心模块
│   ├── spiders/            # 爬虫脚本目录
│   │   ├── positionCategory.py  # 职位分类爬虫
│   │   ├── jobSpider.py        # 职位详情爬虫
│   │   ├── gongsiSpider.py     # 公司信息爬虫
│   │   └── hotPositionSpider.py # 热门职位爬虫
│   ├── items.py            # 数据字段定义
│   ├── pipelines.py        # 数据存储管道（MongoDB/Redis）
│   ├── settings.py         # 爬虫配置（反爬、数据库连接等）
│   └── main.py             # 爬虫启动入口
├── test/                   # 测试脚本目录
│   ├── demoMongodb.py      # MongoDB连接测试
│   └── demoRedis.py        # Redis操作测试
├── README.md               # 项目基础说明
├── scrapy.cfg              # Scrapy项目配置文件
└── 反爬机制.md             # 反爬策略详细说明
```

## 三、实验核心内容
### 1. 数据爬取功能
#### （1）职位分类爬取（positionCategory.py）
- 爬取目标：拉勾网全量职位分类（如技术、产品、运营等）。
- 数据存储：同步存入 Redis（临时缓存）和 MongoDB（持久化存储）。
- 核心逻辑：通过 Scrapy 爬虫解析页面分类标签，提取分类名称、链接等字段，通过管道类写入数据库。

#### （2）职位详情爬取（jobSpider.py）
- 爬取目标：职位名称、公司名称、薪资范围、工作地点、岗位职责、任职要求等。
- 数据存储：直接存入 MongoDB，字段定义参考 items.py。
- 核心逻辑：基于职位分类链接，批量抓取各分类下的职位详情页，解析DOM结构提取目标字段。

#### （3）公司信息爬取（gongsiSpider.py）
- 爬取目标：公司名称、行业类型、规模、融资阶段等企业信息。
- 开发状态：基础框架已搭建，部分解析逻辑需完善。
- 数据存储：支持存入 MongoDB，需配合 pipelines.py 配置。

### 2. 数据存储设计
#### （1）MongoDB 存储
- 连接配置：默认地址 192.168.65.119:27017，可在 settings.py 中修改。
- 集合设计：按数据类型分集合存储（职位分类集合、职位详情集合、公司信息集合）。
- 实现方式：通过 pipelines.py 中的 MongoPipeline 类，接收 Scrapy 爬虫产出的 Item 数据，批量插入 MongoDB。

#### （2）Redis 存储
- 连接配置：默认地址 192.168.65.119，端口默认 6379。
- 存储内容：主要缓存职位分类数据，用于后续职位详情爬取的链接溯源。
- 实现方式：通过 Redis 客户端设置键值对（分类名称-分类链接），支持快速查询。

### 3. 数据处理功能
#### （1）MongoDB 转 CSV（MongodbToCsv.py）
- 功能：将 MongoDB 中指定集合的数据导出为 CSV 文件，支持按条件筛选（如职位名称）。
- 核心方法：`read_delete_by_time_save_to_csv()`，可指定查询条件和导出路径。

#### （2）CSV 缺失值处理（DataProcessing.py）
- 填充缺失值：`fill_missing_value()` 方法，支持按字段类型（数值/字符串）填充默认值或均值。
- 删除缺失值：`delete_missing_value()` 方法，删除包含缺失值的行数据。

### 4. 反爬机制
#### （1）User-Agent 随机切换
- 实现：通过 `randomlyGetUserAgent()` 函数，从 User-Agent 列表中随机选择请求头。
- 作用：避免因固定 User-Agent 被服务器识别为爬虫。

#### （2）Cookie 动态配置
- 策略：支持禁用 Cookie 或周期性更新 Cookie，具体配置参考 `反爬机制.md`。
- 作用：防止因 Cookie 过期或固定 Cookie 触发反爬规则。

#### （3）延时访问控制
- 实现：通过线程休眠（`time.sleep()`）控制请求频率，避免高频次访问。
- 配置：可在 settings.py 中设置 `DOWNLOAD_DELAY` 参数调整延时时间。

## 四、运行步骤
### 1. 环境准备
#### （1）依赖库安装
执行以下命令安装所需 Python 库：
```bash
pip install scrapy pymongo redis pandas
```

#### （2）数据库启动
- 启动 MongoDB 服务：确保 MongoDB 服务已启动，且端口 27017 可访问。
- 启动 Redis 服务：确保 Redis 服务已启动，且端口 6379 可访问。
- 配置核对：确认 settings.py 中 MongoDB、Redis 的连接地址与本地服务一致。

### 2. 爬虫运行
#### （1）启动职位分类爬虫
1. 打开 lagou/main.py 文件。
2. 注释默认启动命令，解除 `positionCategory` 爬虫注释：
   ```python
   # cmd.execute('scrapy crawl job'.split())  # 注释原有命令
   cmd.execute("scrapy crawl positionCategory".split())  # 启用职位分类爬虫
   ```
3. 运行 main.py：
   ```bash
   python lagou/main.py
   ```

#### （2）启动职位详情爬虫
1. 打开 lagou/main.py 文件，确保默认命令启用：
   ```python
   cmd.execute('scrapy crawl job'.split())  # job 对应 jobSpider.py
   ```
2. 运行 main.py，爬虫将基于职位分类数据批量爬取职位详情。

#### （3）启动公司信息爬虫
1. 完善 gongsiSpider.py 中的页面解析逻辑（补充字段提取规则）。
2. 在 settings.py 中注册爬虫名称。
3. 在 main.py 中添加启动命令并运行。

### 3. 数据处理操作
#### （1）MongoDB 数据导出为 CSV
1. 打开 MongoDb2Csv/MongodbToCsv.py。
2. 调用 `read_delete_by_time_save_to_csv()` 方法，指定查询条件（如职位名称）：
   ```python
   if __name__ == "__main__":
       a = MongodbToCsv()
       a.read_delete_by_time_save_to_csv('美术设计师2d3d')  # 导出“美术设计师2d3d”相关数据
   ```
3. 运行脚本，CSV 文件将保存至指定路径（默认脚本同级目录）。

#### （2）CSV 缺失值处理
1. 打开 DataProcess/DataProcessing.py。
2. 实例化 DataProcessing 类，调用对应方法：
   ```python
   if __name__ == "__main__":
       dp = DataProcessing('input.csv', 'output.csv')  # 输入CSV路径、输出CSV路径
       dp.fill_missing_value()  # 填充缺失值
       # dp.delete_missing_value()  # 或删除缺失值
   ```
3. 运行脚本，处理后的 CSV 保存至输出路径。

### 4. 测试脚本运行
- MongoDB 连接测试：运行 test/demoMongodb.py，验证数据库连接与查询功能。
- Redis 操作测试：运行 test/demoRedis.py，验证 Redis 键值对存储与读取。

## 五、注意事项
1. 爬取速率：避免高频次请求，建议在 settings.py 中设置 `DOWNLOAD_DELAY = 2`（延时2秒）。
2. 数据字段：新增字段需在 items.py 中定义，同时更新 pipelines.py 中的存储逻辑。
3. 反爬调整：若爬取失败，参考 `反爬机制.md` 调整 User-Agent 列表、延时时间或 Cookie 策略。
4. 数据库配置：若本地 MongoDB/Redis 端口或地址修改，需同步更新 settings.py 中的连接参数。

要不要我帮你生成一份 **项目快速启动 Checklist.md**，包含环境配置、核心命令、常见问题排查等关键信息，方便快速上手使用？